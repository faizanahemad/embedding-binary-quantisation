\section{Experimental Setup}
\label{sec:experiments}

We evaluated our approach using a suite of benchmark retrieval datasets from the MTEB (Massive Text Embedding Benchmark) library~\cite{muennighoff2022mteb}, including ArguAna~\cite{wachsmuth-etal-2018-retrieval, thakur2021beir}, CQADupstackTexRetrieval~\cite{hoogeveen2015cqadupstack}, ClimateFEVER~\cite{diggelmann2020climatefever}, DBPedia~\cite{hasibi2017dbpedia}, FEVER~\cite{thorne2018fever}, FiQA2018~\cite{thakur2021beir}, HotpotQA~\cite{yang2018hotpotqa}, MSMARCO~\cite{nguyen2016ms}, NFCorpus~\cite{boteva2016full}, NQ (Natural Questions)~\cite{kwiatkowski2019natural}, QuoraRetrieval~\cite{iyer2017first, thakur2021beir}, SCIDOCS~\cite{specter2020cohan}, SciFact~\cite{wadden2020fact}, TREC-COVID~\cite{voorhees2021trec, thakur2021beir}, and Touche2020~\cite{bondarenko2020overview}.


For our experiments, we employed two distinct Transformer models: MiniLM~\cite{minilm, reimers-2019-sentence-bert}, a compact 12-layer model with 384-dimensional embeddings derived through knowledge distillation, and Modern BERT (MB)~\cite{modernbert, nussbaum2024nomic}, a more recent 22-layer architecture with 768-dimensional embeddings incorporating state-of-the-art design principles. This selection allows us to evaluate our approach across different model scales and architectural generations, from efficient distilled models to modern high-capacity architectures.
Demonstrating robust results on both a smaller, older architecture and a more recent, higher-capacity model indicates that practitioners can attain substantial compression benefits—via quantization and nested dimensionality reduction—across diverse Transformer-based architectures.


We compared our approach against the following baselines: \textbf{Original Model (FP32)} (full-precision 32-bit floating point embeddings), \textbf{FP16} (half-precision 16-bit), \textbf{Int8} (8-bit integer quantization), and \textbf{Simple Threshold Quantization} (basic 2-bit, 1.5-bit, and 1-bit quantization using fixed thresholds without learned transformations or Matryoshka representation).

The training process optimizes our Matryoshka model $\mathcal{M}$ initialized from a pretrained model $\mathcal{E}$. 
For each batch, normalized base embeddings are transformed to produce both non-quantized and quantized representations at different dimension levels. 
The training objective combines multiple weighted loss terms: similarity preservation losses ($\mathcal{L}_{\text{sim}}$, $\mathcal{L}_{\text{kl}}$, $\mathcal{L}_{\text{rank}}$, $\mathcal{L}_{\text{contrast}}$), orthogonality loss for unique information across levels, information bottleneck loss to concentrate important features early, and quantization loss to facilitate discretization. 
Quantization thresholds are initialized using percentile-based statistics and updated with momentum. We used AdamW optimizer with $\eta = 1 \times 10^{-4}$, warm-up scheduling, and gradient clipping, training for 5 epochs. 
For evaluation, we used NDCG@10 which measures ranking quality by comparing the relevance scores of retrieved documents against an ideal ranking, normalized to [0,1].
