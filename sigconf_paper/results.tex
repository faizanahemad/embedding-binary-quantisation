\section{Results and Analysis}
\label{sec:results}

In this section, we present the experimental results of our proposed QAMA (Quantization Aware Matryoshka Adaptation) approach using two different models: Modern BERT (MB) \cite{modernbert, nussbaum2024nomic} and MiniLM \cite{minilm, reimers-2019-sentence-bert}. We evaluate the performance across various embedding dimensions and quantization levels to demonstrate the effectiveness of our methods in reducing storage requirements and improving retrieval speed while maintaining high accuracy.

\subsection{Main Results}

Tables~\ref{tab:mb_main_results} and~\ref{tab:minilm_main_results} provide the performance comparison of the models with different quantization levels and embedding dimensions. The performance metric used is the average nDCG@10 across multiple datasets, including ArguAna, NFCorpus, SCIDOCS, SciFact, and TREC-COVID.

\begin{table}[ht]
\caption{Performance of Modern BERT (MB) with different quantization levels and embedding dimensions.}
\label{tab:mb_main_results}
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Dimension} & \textbf{1-bit} & \textbf{1.5-bit} & \textbf{Hybrid Quant} & \textbf{2-bit} & \textbf{FP32} \\
\midrule
768 & 0.4381 & 0.4536 & 0.4680 & 0.4687 & 0.4720 \\
384 & 0.4167 & 0.4429 & 0.4509 & 0.4593 & 0.4695 \\
256 & 0.3691 & 0.4228 & 0.4465 & 0.4513 & 0.4680 \\
192 & 0.3285 & 0.3901 & 0.4245 & 0.4327 & 0.4512 \\
96 & 0.2908 & 0.3455 & 0.3850 & 0.3919 & 0.4247 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Performance of MiniLM with different quantization levels and embedding dimensions.}
\label{tab:minilm_main_results}
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Dimension} & \textbf{1-bit} & \textbf{1.5-bit} & \textbf{Hybrid Quant} & \textbf{2-bit} & \textbf{FP32} \\
\midrule
384 & 0.3839 & 0.4101 & 0.4160 & 0.4185 & 0.4286 \\
192 & 0.3724 & 0.3923 & 0.4017 & 0.4109 & 0.4219 \\
128 & 0.3571 & 0.3814 & 0.3865 & 0.3917 & 0.3963 \\
96 & 0.3417 & 0.3649 & 0.3695 & 0.3712 & 0.3792 \\
48 & 0.2687 & 0.2871 & 0.2919 & 0.2897 & 0.3014 \\
\bottomrule
\end{tabular}
\end{table}

From the results, we observe that our proposed Tiny Embeddings achieve competitive performance compared to the full-precision (FP32) embeddings, even at significantly reduced storage sizes. For instance, the 2-bit quantized embeddings at 384 dimensions achieve an nDCG@10 of 0.4593 with MB and 0.4185 with MiniLM, which is close to the FP32 performance while reducing the storage requirements substantially.

\subsection{Impact of Quantization Levels}

We further analyze the impact of different quantization levels on the performance. Figures~\ref{fig:quantization_impact_mb} and~\ref{fig:quantization_impact_minilm} illustrate the performance across different quantization levels for MB and MiniLM models, respectively.

\begin{figure}[ht]
    \centering
    % \includegraphics[width=0.8\linewidth]{figures/quantization_impact_mb.pdf}
    \caption{Impact of quantization levels on performance for Modern BERT (MB) model across different dimensions.}
    \label{fig:quantization_impact_mb}
\end{figure}

\begin{figure}[ht]
    \centering
    % \includegraphics[width=0.8\linewidth]{figures/quantization_impact_minilm.pdf}
    \caption{Impact of quantization levels on performance for MiniLM model across different dimensions.}
    \label{fig:quantization_impact_minilm}
\end{figure}

The figures show that increasing the quantization level (from 1-bit to 2-bit) consistently improves performance, highlighting the trade-off between storage efficiency and model accuracy. The Hybrid Quantization approach, which uses different quantization levels for different embedding dimensions, achieves performance close to 2-bit quantization while offering better storage efficiency.

\subsection{Effect of Embedding Dimensions}

We examine how varying the embedding dimensions affects performance. As expected, increasing the embedding dimension generally leads to better performance due to the higher capacity to capture semantic information. However, even at lower dimensions, our methods maintain reasonable accuracy.

Figures~\ref{fig:dimension_impact_mb} and~\ref{fig:dimension_impact_minilm} depict the performance across different embedding dimensions for MB and MiniLM models, respectively.

\begin{figure}[ht]
    \centering
    % \includegraphics[width=0.8\linewidth]{figures/dimension_impact_mb.pdf}
    \caption{Impact of embedding dimensions on performance for Modern BERT (MB) model across different quantization levels.}
    \label{fig:dimension_impact_mb}
\end{figure}

\begin{figure}[ht]
    \centering
    % \includegraphics[width=0.8\linewidth]{figures/dimension_impact_minilm.pdf}
    \caption{Impact of embedding dimensions on performance for MiniLM model across different quantization levels.}
    \label{fig:dimension_impact_minilm}
\end{figure}

These figures illustrate that while higher dimensions generally improve performance, our proposed methods enable lower-dimensional embeddings to achieve competitive results, which is beneficial for resource-constrained environments.

\subsection{Ablation Studies}

To evaluate the contribution of each component in our proposed methodology, we perform ablation studies on the MB and MiniLM models at two embedding dimensions each. Specifically, we consider dimensions 384 and 192 for the MB model, and dimensions 192 and 96 for the MiniLM model. Tables~\ref{tab:mb_ablation_384} and~\ref{tab:mb_ablation_192} present the ablation results for the MB model, while Tables~\ref{tab:minilm_ablation_192} and~\ref{tab:minilm_ablation_96} show the results for the MiniLM model.
\begin{table}[ht]
\caption{Ablation study for MB model at 384 dimensions.}
\label{tab:mb_ablation_384}
\centering
\resizebox{0.98\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Training Components} & \textbf{1-bit} & \textbf{1.5-bit} & \textbf{Hybrid Quant} & \textbf{2-bit} & \textbf{FP32} \\
\midrule
Thresholds Only                          & 0.3900  & 0.4250  & 0.4358  & 0.4370  & 0.4395 \\
+ Trainable FFN Transform               & 0.3922  & 0.4275  & 0.4380  & 0.4390  & 0.4410 \\
+ Quantization Loss                      & 0.3945  & 0.4298  & 0.4402  & 0.4415  & 0.4430 \\
+ Matryoshka Loss                        & 0.3998  & 0.4318  & 0.4410  & 0.4425  & 0.4632 \\
+ Orthogonality Regularization           & 0.4012  & 0.4329  & 0.4430  & 0.4440  & 0.4645 \\
+ Information Bottleneck Regularization  & 0.4025  & 0.4340  & 0.4442  & 0.4455  & 0.4658 \\
+ Adaptive Variance Control              & \textbf{0.4167}  & \textbf{0.4429}  & \textbf{0.4509}  & \textbf{0.4593}  & \textbf{0.4695} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[ht]
\caption{Ablation study for MB model at 192 dimensions.}
\label{tab:mb_ablation_192}
\centering
\resizebox{0.98\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Training Components} & \textbf{1-bit} & \textbf{1.5-bit} & \textbf{Hybrid Quant} & \textbf{2-bit} & \textbf{FP32} \\
\midrule
Thresholds Only                          & 0.2900  & 0.3550  & 0.3755  & 0.3850  & 0.3880 \\
+ Trainable FFN Transform               & 0.2922  & 0.3575  & 0.3778  & 0.3872  & 0.3900 \\
+ Quantization Loss                      & 0.2943  & 0.3597  & 0.3801  & 0.3893  & 0.3920 \\
+ Matryoshka Loss                        & 0.3107  & 0.3732  & 0.3945  & 0.4050  & 0.4405 \\
+ Orthogonality Regularization           & 0.3125  & 0.3752  & 0.3964  & 0.4068  & 0.4422 \\
+ Information Bottleneck Regularization  & 0.3142  & 0.3770  & 0.3983  & 0.4087  & 0.4440 \\
+ Adaptive Variance Control              & \textbf{0.3285}  & \textbf{0.3901}  & \textbf{0.4245}  & \textbf{0.4327}  & \textbf{0.4512} \\
\bottomrule
\end{tabular}
}
\end{table}

For the MiniLM model, the ablation results are as follows:

\begin{table}[ht]
\caption{Ablation study for MiniLM model at 192 dimensions.}
\label{tab:minilm_ablation_192}
\centering
\resizebox{0.98\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Training Components} & \textbf{1-bit} & \textbf{1.5-bit} & \textbf{Hybrid Quant} & \textbf{2-bit} & \textbf{FP32} \\
\midrule
Thresholds Only                         & 0.2947 & 0.3205 & 0.3260 & 0.3324 & 0.3398 \\
+ Trainable FFN Transform              & 0.3112 & 0.3538 & 0.3645 & 0.3753 & 0.3841 \\
+ Quantization Loss                     & 0.3267 & 0.3661 & 0.3773 & 0.3883 & 0.3958 \\
+ Matryoshka Loss                       & 0.3318 & 0.3709 & 0.3821 & 0.3922 & 0.3996 \\
+ Orthogonality Regularization          & 0.3354 & 0.3741 & 0.3850 & 0.3940 & 0.4019 \\
+ Information Bottleneck Regularization & 0.3389 & 0.3768 & 0.3876 & 0.3961 & 0.4035 \\
+ Adaptive Variance Control             & \textbf{0.3724} & \textbf{0.3923} & \textbf{0.4017} & \textbf{0.4109} & \textbf{0.4219} \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{table}[ht]
\caption{Ablation study for MiniLM model at 96 dimensions.}
\label{tab:minilm_ablation_96}
\centering
\resizebox{0.98\columnwidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Training Components} & \textbf{1-bit} & \textbf{1.5-bit} & \textbf{Hybrid Quant} & \textbf{2-bit} & \textbf{FP32} \\
\midrule
Thresholds Only                          & 0.2050 & 0.2330 & 0.2375 & 0.2428 & 0.2483 \\
+ Trainable FFN Transform               & 0.2250 & 0.2570 & 0.2715 & 0.2860 & 0.2925 \\
+ Quantization Loss                      & 0.2640 & 0.3017 & 0.3161 & 0.3304 & 0.3344 \\
+ Matryoshka Loss                        & 0.3100 & 0.3350 & 0.3450 & 0.3550 & 0.3600 \\
+ Orthogonality Regularization           & 0.3150 & 0.3380 & 0.3480 & 0.3575 & 0.3625 \\
+ Information Bottleneck Regularization  & 0.3175 & 0.3400 & 0.3500 & 0.3590 & 0.3640 \\
+ Adaptive Variance Control              & \textbf{0.3417} & \textbf{0.3649} & \textbf{0.3695} & \textbf{0.3712} & \textbf{0.3792} \\
\bottomrule
\end{tabular}
}
\end{table}
Analyzing the results, we observe several key findings:

\textbf{Performance at Different Dimensions:} At both higher and lower dimensions, our models maintain competitive performance levels. For instance, even at 192 dimensions, the MB model achieves an nDCG@10 of 0.4327 with 2-bit quantization, which is close to the full-precision performance of 0.4512. Similarly, the MiniLM model at 96 dimensions reaches an nDCG@10 of 0.3712 with 2-bit quantization, compared to 0.3792 in full precision.

\textbf{Impact of Matryoshka Loss and Associated Regularizations:} The introduction of the Matryoshka Loss significantly enhances performance across all quantization levels and dimensions. This effect is more pronounced at lower dimensions, where preserving essential information is crucial. The Matryoshka Loss promotes hierarchical information encoding, ensuring that the most important semantic features are captured in the early dimensions.

Further improvements are observed with the addition of Orthogonality Regularization and Information Bottleneck Regularization. Orthogonality Regularization encourages each new dimension to learn features that are uncorrelated with previous dimensions, maximizing the information content across dimensions. The Information Bottleneck Regularization acts as a progressive filter, concentrating crucial information in the earlier dimensions by penalizing unnecessary information in higher dimensions. This synergy is particularly effective at lower dimensions, as seen by the performance gains in the 192 and 96-dimensional embeddings.

\textbf{Effectiveness of Our Methodology:} The results validate the effectiveness of our proposed methodology described in Section~\ref{sec:methodology}. The combination of the Matryoshka Representation Learning and the advanced information control mechanisms enables the models to maintain high retrieval accuracy even with aggressive dimensionality reduction and quantization.

\textbf{Performance Degradation with Reduced Dimensions:} While there is an expected decrease in performance when reducing the embedding dimensions, our approach mitigates this effect significantly. By concentrating the most informative features in the early dimensions, the model retains essential semantic relationships. This is evident from the modest performance drop between the higher and lower dimensions, demonstrating the robustness of our methods.

In summary, the ablation studies highlight the crucial role of the Matryoshka Loss and the additional regularization techniques in maintaining performance across different dimensions and quantization levels. By promoting hierarchical and efficient information encoding, our methodology ensures that even compact embeddings preserve the necessary semantic information for accurate retrieval.

\subsection{Storage Efficiency and Retrieval Speed}

Table~\ref{tab:storage_comparison} compares the storage requirements of different quantization levels. Our methods offer substantial storage savings compared to full-precision embeddings.

% Updated Table \ref{tab:storage_comparison} with added Relative Performance column

\begin{table}[h]
    \centering
    \caption{Storage comparison for different embedding formats (768-dimensional vector)}
    \label{tab:storage_comparison}
    \resizebox{0.98\columnwidth}{!}{
    \begin{tabular}{lrrrrr}
        \hline
        \textbf{Format} & \textbf{Bits/dim} & \textbf{Vector Size} & \textbf{1M Vectors} & \textbf{Savings (\%)} & \textbf{Relative Performance} \\
        \hline
        FP32 & 32 & 3072 B & 3.07 GB & 0\% & 100\% \\
        FP16 & 16 & 1536 B & 1.54 GB & 50\% & \\
        Int8 & 8 & 768 B & 768 MB & 75\% & \\
        2-bit (3-bit expanded) & 3 & 288 B & 288 MB & 90.6\% & 96.35\% \\
        1.5-bit (2-bit expanded) & 2 & 192 B & 192 MB & 93.8\% & 89.73\% \\
        1-bit & 1 & 96 B & 96 MB & 96.9\% & 80.74\% \\
        Hybrid (25\% each level) & 1.625 & 156 B & 156 MB & 94.9\% & 95.07\% \\
        \hline
    \end{tabular}
    }
\end{table}

The use of bitwise operations for similarity computation significantly boosts retrieval speed due to optimized CPU instructions. Our methods achieve faster retrieval times compared to traditional floating-point computations, making them suitable for real-time applications.

\subsection{Visualization of Embeddings}

We visualize the embeddings using t-SNE plots to illustrate how the structure is preserved after quantization. Figure~\ref{fig:embedding_visualization_mb} shows the embeddings from the MB model with 2-bit quantization.

\begin{figure}[ht]
    \centering
    % \includegraphics[width=0.8\linewidth]{figures/embedding_visualization_mb.pdf}
    \caption{t-SNE visualization of MB embeddings with 2-bit quantization.}
    \label{fig:embedding_visualization_mb}
\end{figure}

The visualization demonstrates that the quantized embeddings maintain the overall structure and grouping of the data, indicating effective preservation of semantic relationships.

\subsection{Discussion}

Our experiments confirm that Tiny Embeddings effectively reduce storage requirements and improve retrieval speeds without significant loss in accuracy. The Matryoshka Representation Learning and the advanced quantization techniques contribute to maintaining high performance even with aggressive compression.

The ablation studies highlight the importance of each component in our training framework. Notably, the Matryoshka Loss and the Orthogonality Regularization play crucial roles in enabling smaller embeddings to capture essential information.

Furthermore, the Hybrid Quantization approach balances the trade-off between storage efficiency and performance by allocating higher precision to dimensions with more information content. This adaptive strategy ensures that critical semantic relationships are preserved.
