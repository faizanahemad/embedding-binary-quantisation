\documentclass[sigconf,natbib=true,anonymous=true]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
%\documentclass[manuscript,screen,review]{acmart}
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2025}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
\acmBooktitle{Woodstock '25: ACM Symposium on Neural Gaze Detection,
 June 03--05, 2025, Woodstock, NY} 
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}


%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.

\usepackage{xcolor}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\setlength{\marginparwidth}{2.5cm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}    % For math environments and \text
\usepackage{amssymb}    % For mathematical symbols
\usepackage{bbm}        % For \mathbbm
\usepackage{graphicx}   % For \includegraphics
\usepackage{totpages}

% Add this in the preamble (after \usepackage declarations)
\newcommand{\redtodo}[1]{\todo[inline, color=red]{\textbf{TODO:} #1}}

\begin{document}

% \title[Tiny Embeddings]{Tiny Embeddings: Leveraging Matryoshka Learning, Quantization, and Bitwise Operations for Reduced Storage and Improved Retrieval Speed}

\title[Quantization Aware Matryoshka Adaptation]{Quantization Aware Matryoshka Adaptation: Leveraging Matryoshka Learning, Quantization, and Bitwise Operations for Reduced Storage and Improved Retrieval Speed}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.

% \begin{abstract}  
%   High-dimensional embeddings are powerful tools in natural language processing and information retrieval, capturing intricate semantic relationships. 
%   However, their large size leads to increased storage requirements and slower retrieval speeds, posing challenges for deployment in resource-constrained environments. 
%   In this paper, we introduce \textbf{Tiny Embeddings}, a novel approach that combines \textit{Matryoshka Representation Learning}, advanced quantization techniques, and efficient bitwise operations to create compact and efficient embeddings. 
%   By augmenting existing models with additional feedforward neural network layers and specialized loss functions, we enforce the Matryoshka property, enabling the use of lower-dimensional embeddings without significant performance loss. 
%   We explore 0.5-bit, 1-bit, 1.5-bit, and 2-bit quantization levels to reduce storage by decreasing numeric precision. 
%   To further enhance retrieval speed, we extend the use of Hamming distance and similarity to multi-bit quantized embeddings by mapping them to binary representations, allowing efficient computation using bitwise CPU instructions like XOR, NOT, and POPCOUNT. 
%   Our hybrid architecture applies higher precision quantization to dimensions with more information content and lower precision to those with less, optimizing storage and performance. 
%   Experimental results demonstrate that our approach significantly reduces storage requirements and improves retrieval speeds while maintaining competitive accuracy compared to uncompressed embeddings.  
  
% \end{abstract} 

\begin{abstract}
  We introduce \emph{Quantization Aware Matryoshka Adaptation (QAMA)}, a unified framework for creating compact yet semantically rich embeddings through \textbf{Matryoshka Representation Learning} and multi-level quantization. 
  Our approach learns nested embeddings that gracefully shrink to smaller dimensional subsets and leverages bitwise operations (XOR, NOT, POPCOUNT) for efficient retrieval. 
  By augmenting transformer-based encoders with lightweight feedforward layers and specialized regularization (Matryoshka Loss, Orthogonality, Information Bottleneck, and Quantization Loss), we produce quantization-friendly representations that preserve essential information in early dimensions.

  We explore 0.5-bit, 1-bit, 1.5-bit, and 2-bit quantization levels, as well as a Hybrid Quantization scheme that adaptively allocates higher precision to dimensions with greater information content. 
  Across extensive evaluations on Modern BERT (MB) and MiniLM models, \textbf{2-bit quantization} and \textbf{Hybrid Quant} consistently recover up to 95--98\% of the original full-precision (FP32) performance while reducing memory usage by over 90\%. 
  Even at low embedding dimensions (e.g., 96--192), QAMAâ€™s hierarchical training ensures that performance remains surprisingly robust, highlighting the effectiveness of our bit-level expansions and nested representation learning.

  Furthermore, our experiments show that end-to-end training with quantization-aware losses yields embeddings that map cleanly into discrete levels, supporting rapid Hamming distance calculations using bitwise operations for large-scale retrieval. 
  Ablation studies reveal that our suggested component losses contributes to preserving semantic fidelity under aggressive compression. 
  Our framework \textbf{QAMA} offers a practical means to store, retrieve, and compute similarities between embeddings efficiently, enabling high-accuracy search with faster retrieval speeds under stringent memory and latency constraints.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging}

\maketitle
\let\clearpage\relax  % Disable automatic page breaks
\input{intro}  
\input{related}  
\input{methodology}
\input{experiments}
\input{results}
\input{conclusion}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
